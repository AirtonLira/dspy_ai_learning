"""
Sentiment Analysis Optimization using MIPROv2

Este m√≥dulo otimiza prompts para classifica√ß√£o de sentimento
usando a estrat√©gia MIPROv2 do DSPy.

Autor: Airton Lira
Data: 2026-01-24
"""

import dspy
from dspy.teleprompt import MIPROv2
from domain.module.sentiment import SentimentClassifier
from domain.evaluation.sentiment_eval import sentiment_dataset
from pathlib import Path
import json
from datetime import datetime


RESULTS_DIR = Path("results")
RESULTS_DIR.mkdir(exist_ok=True)


class SentimentMiproManager:
    """
    Gerenciador de otimiza√ß√£o MIPROv2 para classifica√ß√£o de sentimento.
    
    Atributos:
        trainset: Dataset de treinamento
        base_program: Programa base SentimentClassifier
        compiled_program: Programa otimizado ap√≥s compila√ß√£o
    """
    
    def __init__(self):
        """Inicializa o gerenciador com dataset e programa base."""
        self.trainset = sentiment_dataset()
        self.base_program = None
        self.compiled_program = None
        
        if not self.trainset:
            print("‚ùå Erro: Dataset vazio!")
            return
        
        self.base_program = SentimentClassifier()
        print(f"‚úÖ Gerenciador inicializado com {len(self.trainset)} exemplos")


    def _metric(self, example, pred, trace=None):
        """
        M√©trica de avalia√ß√£o: acur√°cia simples.
        
        Compara o sentimento predito com o esperado (case-insensitive).
        
        Args:
            example: Exemplo do dataset com campo 'sentiment'
            pred: Predi√ß√£o do modelo com campo 'sentiment'
            trace: Trace do DSPy (n√£o usado aqui)
            
        Returns:
            int: 1 se correto, 0 se incorreto
        """
        try:
            expected = example.sentiment.lower().strip()
            predicted = pred.sentiment.lower().strip()
            
            is_correct = int(expected == predicted)
            return is_correct
            
        except AttributeError as e:
            print(f"‚ö†Ô∏è  Erro ao acessar campos: {e}")
            return 0


    def run_mipro_optimization(self):
        """
        Executa otimiza√ß√£o MIPROv2 com modo autom√°tico.
        
        Usa auto="medium" para deixar o DSPy decidir automaticamente
        os par√¢metros de otimiza√ß√£o (num_candidates, num_trials).
        
        Returns:
            dspy.ChainOfThought: Programa compilado otimizado ou None se falhar
        """
        
        if self.base_program is None:
            print("‚ùå Erro: Programa base n√£o inicializado")
            return None
        
        if self.trainset is None or len(self.trainset) == 0:
            print("‚ùå Erro: Dataset vazio ou n√£o carregado")
            return None
        
        print("\n" + "="*60)
        print("üöÄ INICIANDO OTIMIZA√á√ÉO MIPROV2")
        print("="*60)
        print(f"   üìä Dataset: {len(self.trainset)} exemplos")
        print(f"   ‚öôÔ∏è  Modo: auto='medium'")
        print(f"   üìà M√©trica: Acur√°cia (sentiment match)")
        print("="*60 + "\n")
        
        try:
            # ‚úÖ CORRIGIDO: usar auto="medium" SEM num_candidates
            # Quando auto √© fornecido, DSPy controla tudo automaticamente
            teleprompter = MIPROv2(
                prompt_model=dspy.settings.lm,
                task_model=dspy.settings.lm,
                metric=self._metric,
                auto="medium"  # ‚úÖ Controla num_candidates e num_trials automaticamente
            )
            
            print("‚è≥ Compilando e otimizando programa...")
            print("   (Este processo pode levar alguns minutos...)\n")
            
            compiled_program = teleprompter.compile(
                student=self.base_program,
                trainset=self.trainset,
                max_bootstrapped_demos=2,
                max_labeled_demos=2
            )
            
            self.compiled_program = compiled_program
            
            print("\n" + "="*60)
            print("‚úÖ OTIMIZA√á√ÉO CONCLU√çDA COM SUCESSO!")
            print("="*60 + "\n")
            
            return compiled_program
            
        except ValueError as e:
            print(f"\n‚ùå ValueError durante otimiza√ß√£o:")
            print(f"   {str(e)}\n")
            return None
            
        except Exception as e:
            print(f"\n‚ùå Erro inesperado: {type(e).__name__}")
            print(f"   {str(e)}\n")
            return None


    def evaluate_compiled_program(self):
        """
        Avalia o programa compilado no dataset de treinamento.
        
        Processa cada exemplo do dataset e calcula acur√°cia geral.
        
        Returns:
            dict: Dicion√°rio com estat√≠sticas de avalia√ß√£o ou None se falhar
        """
        
        if self.compiled_program is None:
            print("‚ùå Erro: Programa compilado n√£o existe")
            return None
        
        if self.trainset is None or len(self.trainset) == 0:
            print("‚ùå Erro: Dataset vazio")
            return None
        
        scores = []
        
        print("\n" + "="*60)
        print("üìä AVALIANDO PROGRAMA COMPILADO")
        print("="*60 + "\n")
        
        for i, example in enumerate(self.trainset):
            try:
                # Fazer predi√ß√£o
                prediction = self.compiled_program(text=example.text)
                
                # Calcular score
                score = self._metric(example, prediction)
                scores.append(score)
                
                # Progresso a cada 10 exemplos
                if (i + 1) % 10 == 0:
                    current_acc = sum(scores[:i+1]) / (i + 1)
                    print(f"   ‚úì {i + 1:3d}/{len(self.trainset)} exemplos | "
                          f"Acur√°cia parcial: {current_acc:.2%}")
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erro ao processar exemplo {i}: {type(e).__name__}")
                scores.append(0)
        
        # Calcular acur√°cia final
        accuracy = sum(scores) / len(scores) if scores else 0
        correct = sum(scores)
        incorrect = len(scores) - correct
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "accuracy": accuracy,
            "total_examples": len(scores),
            "correct": correct,
            "incorrect": incorrect,
            "accuracy_percentage": f"{accuracy:.2%}"
        }
        
        print("\n" + "="*60)
        print("üìà RESULTADOS DA AVALIA√á√ÉO")
        print("="*60)
        print(f"   Acur√°cia:      {accuracy:.2%}")
        print(f"   Corretos:      {correct}/{len(scores)}")
        print(f"   Incorretos:    {incorrect}/{len(scores)}")
        print("="*60 + "\n")
        
        return results


    def save_checkpoint(self, filename="sentiment_mipro_optimized.json"):
        """
        Salva o programa compilado em arquivo.
        
        Args:
            filename (str): Nome do arquivo de sa√≠da
            
        Returns:
            bool: True se sucesso, False se falhar
        """
        
        if self.compiled_program is None:
            print("‚ùå Erro: Nenhum programa compilado para salvar")
            return False
        
        try:
            filepath = RESULTS_DIR / filename
            
            print(f"üíæ Salvando programa compilado...")
            self.compiled_program.save(str(filepath))
            print(f"‚úÖ Programa salvo em: {filepath}\n")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao salvar programa: {type(e).__name__}: {e}\n")
            return False


    def save_results(self, results, filename="mipro_results.json"):
        """
        Salva resultados de avalia√ß√£o em formato JSON.
        
        Args:
            results (dict): Dicion√°rio com resultados
            filename (str): Nome do arquivo de sa√≠da
            
        Returns:
            bool: True se sucesso, False se falhar
        """
        
        if results is None:
            print("‚ö†Ô∏è  Aviso: Resultados vazios, pulando salvamento")
            return False
        
        try:
            filepath = RESULTS_DIR / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Resultados salvos em: {filepath}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao salvar resultados: {type(e).__name__}: {e}")
            return False


def run_optimization():
    """
    Fun√ß√£o principal para executar otimiza√ß√£o MIPROv2.
    
    Orquestra√ß√£o completa:
    1. Inicializa gerenciador com dataset
    2. Executa otimiza√ß√£o MIPROv2
    3. Avalia programa otimizado
    4. Salva resultados e checkpoint
    """
    
    print("\n" + "#"*60)
    print("# OTIMIZA√á√ÉO DE SENTIMENTO COM MIPROV2")
    print("#"*60 + "\n")
    
    # Inicializar gerenciador
    manager = SentimentMiproManager()
    
    if manager.base_program is None:
        print("‚ùå Falha na inicializa√ß√£o. Encerrando.")
        return
    
    # Executar otimiza√ß√£o
    optimized_program = manager.run_mipro_optimization()
    
    if optimized_program is None:
        print("‚ùå Falha na otimiza√ß√£o. Encerrando.")
        return
    
    # Avaliar programa compilado
    results = manager.evaluate_compiled_program()
    
    if results:
        # Salvar resultados
        manager.save_results(results)
        manager.save_checkpoint()
        
        print("\n" + "#"*60)
        print("# ‚úÖ OTIMIZA√á√ÉO CONCLU√çDA COM SUCESSO!")
        print("#"*60)
        print(f"   Acur√°cia Final: {results['accuracy_percentage']}")
        print(f"   Corretos: {results['correct']}/{results['total_examples']}")
        print("#"*60 + "\n")
    else:
        print("\n‚ö†Ô∏è  Avalia√ß√£o n√£o retornou resultados v√°lidos")


if __name__ == "__main__":
    # Para testes diretos
    import os
    from dotenv import load_dotenv
    from utils.config import setup_llm
    
    load_dotenv()
    setup_llm()
    
    run_optimization()
